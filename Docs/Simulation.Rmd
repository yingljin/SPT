---
title: "Simulating spatial-temporal data"
author: "`r Sys.Date()`"
output: 
  html_document:
    self_contained: yes
    number_sections: true
    code_folding: hide
    toc: true
    toc_depth: 3
    toc_float: true
    font: 12pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

set.seed(1026)

library(mgcv)
library(nlme)
library(tidyverse)
library(knitr)
library(kableExtra)
library(mvtnorm)
library(ggpubr)
library(here)

source(here("Code/RandomField.R"))
```


# Generation framework

The simulation, technically, would be established on a continuous space-time domain, even though the final "observations" would be a discrete realization.

Take a Gaussian process for example:

1. The observation is composed of a true latent process and an error process.

$$Y_i(\mathbf{s}, t) = \eta_i(\mathbf{s}, t) +\epsilon_i(\mathbf{s}, t)$$
2. The true latent process is composed of a fixed process and a random (subject-specific) process. 

$$\eta_i(\mathbf{s}, t) = \mu(\mathbf{s}, t)+b_i(\mathbf{s}, t)$$

- $\mu(\mathbf{s}, t)$ is the population mean function, shared across subjects
- $b_i(\mathbf{s}, t)$ is the individual-level random effect

3. The error process is spatially-correlated. Correlation is introduced through a moving average random field: 

$$\epsilon_i(\mathbf{s}, t) =  \frac{1}{N_r}\sum_{\mathbf{s'} \in S_r}Z(\mathbf{s'}, t)$$


where:

- $S_r$ is a neighborhood around $\mathbf{s}$ where the radius is r
- $N_r$ is the number of spacial points within neighborhood $S_r$
- $Z(\mathbf{s'}, t)$ is a white noise process


4. The second outcome is generated from the first outcome, and the correlation is time-varying

$$Y'_i(\mathbf{s},t) = \beta_i(t)Y_i(\mathbf{s},t) + b'_i(\mathbf{s},t) + \epsilon_i(\mathbf{s}, t)$$


# Full data

Follow up on last time, we would like to generate data from the null hypothesis ($Y_1$, $Y_2$ are not correlated with each other), examine p values from LM or permutation test, and calculate their type I error (probability of reject under the null hypothesis) at 95% confidence level.

In previous experiments we realized the type I error from all three methods (LM, permutation test and GLM) was greater than the nominal value. We would like to know the reason behind it. 

## Two iid spatial moving average error

Let's start with two iid variables from the spatial moving average error. That is, $Y_{i1}$ and $Y_{i2}$ are both moving average of a white noise filed using **5 by 5** kernel.

```{r grid}
# set up spcae-time grid
# generate a 2D image of 256 by 256
sid1 <- sid2 <- 1:32
nS <- 32
df_grid <- expand_grid(sid1, sid2) %>%
  mutate(s1 = as.vector(scale(sid1)), s2 = as.vector(scale(sid2))) %>% 
  mutate(dist = sqrt(s1^2+s2^2))
## we would need distance to center for the random effect of Y1

# times of scan
t <- seq(0.2, 1 , by = 0.2)
nT <- length(t)

df_grid <- expand_grid(df_grid, t=t) 
## 32^2*5 = 5120 observations for each subject
```


```{r sim_setup}
N <- 100 # sample size

# container
df_subj <- expand_grid(id = 1:N, df_grid)
df_subj$Y1 <- df_subj$Y2 <- NA
## N * 256^2 * nT = 512000 obesrvations in total

# kernel size for moving average
ma_size1 <- 5
ma_size2 <- 5
```


```{r gen_data1, results='hide'}
pb <- txtProgressBar(min=0, max=N, style = 3)

t1 <- Sys.time()
for(i in 1:N){ # fix a subject
  
  for(this_t in t){ # fix a time point
  
    # generate Y1
    ## a moving average error
    Zmat_it1 <- matrix(rnorm(nS^2, 0, 1), nS, nS)
    Y1_it <- MA_rand_field(ma_size1, Zmat_it1)
    df_subj$Y1[df_subj$id==i & df_subj$t==this_t] <- as.vector(Y1_it)
    
    # generate Y2
    ## a moving average error
    Zmat_it2 <- matrix(rnorm(nS^2, 0, 1), nS, nS)
    Y2_it <- MA_rand_field(ma_size2, Zmat_it2)
    df_subj$Y2[df_subj$id==i & df_subj$t==this_t] <- as.vector(Y2_it)
  }

setTxtProgressBar(pb, i)
}
t2 <- Sys.time()

close(pb)
```


It took `r round(t2-t1, 3)` minutes to generate data for `r N` subjects. Below we show an example of one subject. 


```{r, example1, fig.height=6, fig.width=15}
df_subj %>% 
  pivot_longer(starts_with("Y")) %>%
  filter(id==1) %>% 
  ggplot()+
  geom_tile(aes(x=s1, y=s2, fill = value))+
  facet_grid(cols=vars(t), rows = vars(name))+
  labs(title = "Outcome")
```

### LM

```{r, fig.height=12, fig.width=15}
df_subj %>% 
  filter(id %in% 1:4) %>%
  ggplot(aes(x=Y1, y=Y2))+
  geom_point(size = 0.2)+
  geom_smooth(formula = 'y~x', method = "lm")+
  stat_cor(method = "pearson")+
  facet_grid(rows = vars(id), cols = vars(t))+
  labs(title = "Correlation")
```

Here, even the pearson correlation test for as single subject at each time point is mostly significant (p<0.05). If we fit linear models for each subject at each time point, the false rejection rate is very high. The model seems to think the two outcomes are correlated. 

The model: $Y_{2i}(\mathbf{s}|t) = \beta_{0ti}+\beta_{1ti}Y_{1i}(\mathbf{s}|t)+\epsilon_i(\mathbf{s}|t)$

```{r}
df_subj %>%
  group_by(id, t) %>%
  group_modify(~{
    fit_lm <- lm(Y2~Y1, data = .x)
    data.frame(beta = coef(fit_lm)["Y1"], 
               pval = summary(fit_lm)$coefficients["Y1", "Pr(>|t|)"])
  }) %>%
  mutate(reject = pval < 0.05) %>%
  group_by(t) %>%
  summarise_at("reject", mean)
```


This type I error is very high! Is it solely because the correlation introduced by both sptially-correlated error term? Adding spatial location as covariates did not help (even when the spatial covariates took on complicated forms in gam)

I am still not sure about the data generation scheme. What would "no correlation" be like? In this case, the two outcomes, respectibely, are spatially-correlated within themselves. And the spatial correlation is generated in the same way. Should that indicate the two outcomes are correlated through spatial correlation? Is that why the type 1 error rate is so high? 

I'm also thinking would it be that we have only one subject for each model. Perhaps we should use all subjects to fit the model? But even if that is the case, the slope of Y1 is also significant at all time points. The sae gose 


```{r, eval=FALSE}
df_subj %>% group_by(t) %>%
  group_map(.,~{summary(lm(Y2~Y1, data = .x))})

fit_lm <- lm(Y2~Y1, data = df_subj %>% filter(t==0.2))
summary(fit_lm)
```


```{r eval=FALSE}
fit_gam <- gam(Y2~Y1+s(s1)+s(s2), data = df_subj %>% filter(id==1 & t==0.4))
summary(fit_gam)
```


## Permutation test 

I have decided to code up my own permutation test because the packages (coin, lmPerm) either couldn't handle the data or have a lot of other contingencies. 

I followed this [reference](https://bookdown.org/curleyjp0/psy317l_guides5/permutation-testing.html#correlation-coefficient-permutation-tests) on permutation test of correlation. 


```{r perm_test, class.source='fold-show', results='hide'}
# number of permutation
M <- 1000 
# containers
obs_T_mat <- obs_beta_mat <- array(NA, dim = c(length(t), N)) # dims: time, subject
T_mat <- beta_mat <- array(NA, dim = c(length(t), N, M)) # dims: time, subject, permutation

# permutation

pb <- txtProgressBar(0, N, style = 3)
t1 <- Sys.time()

for(i in 1: N){
  for(tid in seq_along(t)){
    this_t <- t[tid]
    df_it <- df_subj %>% filter(id==i & t==this_t)
    
    # observed 
    this_lm <- summary(lm(Y2~Y1, data = df_it))$coefficients
    obs_T_mat[tid, i] <- this_lm["Y1", "t value"]
    obs_beta_mat[tid, i] <- this_lm["Y1", "Estimate"]
  
    # permutation 
    for(m in 1:M){
      df_it_m <- df_it
      df_it_m$Y2_perm <- sample(df_it_m$Y2) # shuffle Y2, fix Y1
      perm_lm <-  summary(lm(Y2_perm~Y1, data = df_it_m))$coefficients
      T_mat[tid, i, m] <- perm_lm["Y1", "t value"]
      beta_mat[tid, i, m] <- perm_lm["Y1", "Estimate"]
    }

  }
  
  setTxtProgressBar(pb, i)

}
t2 <- Sys.time()
close(pb)
```

The permutation rest took `r round(t2-t1, 2)` minutes. 

Below is the distribution of permutated t statistics and coefficients of subject 1

```{r, fig.height=3, fig.width=15}
data.frame(
  time = t,
  obs_T = obs_T_mat[,1], 
  T_mat[,1,]) %>%
  pivot_longer(starts_with("X"), names_prefix = "X") %>%
  ggplot()+
  geom_histogram(aes(x=value), bins = 30)+
  geom_vline(aes(xintercept = obs_T), col = "red")+
  facet_grid(cols = vars(time))+
  labs(title = "T statistics")

```

```{r, fig.height=3, fig.width=15}
data.frame(
  time = t,
  obs_beta = obs_beta_mat[,1], 
  beta_mat[,1,]) %>%
  pivot_longer(starts_with("X"), names_prefix = "X") %>%
  ggplot()+
  geom_histogram(aes(x=value), bins = 30)+
  geom_vline(aes(xintercept = obs_beta), col = "red")+
  facet_grid(cols = vars(time))+
  labs(title = "Slope estimates")

```

Below the type 1 error rate. We table absolute values because this is a two-sided test. 

```{r}
# using t statistics
pval_mat <- matrix(NA, nT, N)
for(i in 1:N){
  for(j in 1:nT){
    pval_mat[j, i] <- mean(abs(T_mat[j, i, ]) > abs(obs_T_mat[j, i]))
  }
}

data.frame(t = t, reject = apply(pval_mat < 0.05, 1, mean))
```

This false rejection rates are exactly the same as linear regression! This is saying the permutation test didn't do anything. Permutation test is robust against non-normality of the outcome. In this case, the outcome is indeed normal. 

Adding spatial index and interaction did not help with the type I error a lot. Only slightly. 

### Can I made permutation test robust against spatial correlation

If I include all subjects at a specific time point. I'd fix the location index and permute only across subject. In this case I'll achieve a p-value at each time and location.

```{r, class.source='fold-show', results='hide'}
df_subj$sp_id <- rep(1:nS^2, N*nT)
# length(rep(1:nS^2, N*nT))

# number of permutation
M <- 1000 
# containers
obs_T_mat <- obs_beta_mat <- array(NA, dim = c(length(t), nS^2)) # dims: time, location
T_mat <- beta_mat <- array(NA, dim = c(length(t), nS^2, M)) # dims: time, subject, permutation

# permutation

pb <- txtProgressBar(0, nS^2, style = 3)
t1 <- Sys.time()

for(sid in 1: nS^2){
  for(tid in seq_along(t)){
    this_t <- t[tid]
    df_spt <- df_subj %>% filter(sp_id==sid & t==this_t)
    
    # observed 
    this_lm <- summary(lm(Y2~Y1, data = df_spt))$coefficients
    obs_T_mat[tid, sid] <- this_lm["Y1", "t value"]
    obs_beta_mat[tid, sid] <- this_lm["Y1", "Estimate"]
  
    # permutation 
    for(m in 1:M){
      df_spt_m <- df_spt
      df_spt_m$Y2_perm <- sample(df_spt_m$Y2) # shuffle Y2, fix Y1
      perm_lm <-  summary(lm(Y2_perm~Y1, data = df_spt_m))$coefficients
      T_mat[tid, sid, m] <- perm_lm["Y1", "t value"]
      beta_mat[tid, sid, m] <- perm_lm["Y1", "Estimate"]
    }

  }
  
  setTxtProgressBar(pb, sid)

}
t2 <- Sys.time()
close(pb)
```

```{r, fig.height=3, fig.width=15}
data.frame(
  time = t,
  obs_T = obs_T_mat[,1], 
  T_mat[,1,]) %>%
  pivot_longer(starts_with("X"), names_prefix = "X") %>%
  ggplot()+
  geom_histogram(aes(x=value), bins = 30)+
  geom_vline(aes(xintercept = obs_T), col = "red")+
  facet_grid(cols = vars(time))+
  labs(title = "T statistics")
```


```{r, fig.height=3, fig.width=15}
data.frame(
  time = t,
  obs_beta = obs_beta_mat[,1], 
  beta_mat[,1,]) %>%
  pivot_longer(starts_with("X"), names_prefix = "X") %>%
  ggplot()+
  geom_histogram(aes(x=value), bins = 30)+
  geom_vline(aes(xintercept = obs_beta), col = "red")+
  facet_grid(cols = vars(time))+
  labs(title = "Slope estimates")

```


```{r}
# using t statistics
pval_mat <- matrix(NA, nT, N)
for(i in 1:N){
  for(j in 1:nT){
    pval_mat[j, i] <- mean(abs(T_mat[j, i, ]) > abs(obs_T_mat[j, i]))
  }
}

data.frame(t = t, reject = apply(pval_mat < 0.05, 1, mean))
```


## Bootstrap

I wanna try bootstrap test

```{r bootstrap, results='hide'}
obs_T_mat2 <- obs_beta_mat2 <- array(NA, dim = c(length(t), N)) # dims: time, subject
T_mat2 <- beta_mat2 <- array(NA, dim = c(length(t), N, M)) # dims: time, subject, permutation

# bootstrap
pb <- txtProgressBar(0, N, style = 3)
t1 <- Sys.time()

for(i in 1: N){
  for(tid in seq_along(t)){
    this_t <- t[tid]
    df_it <- df_subj %>% filter(id==i & t==this_t)
    
    # observed 
    this_lm <- summary(lm(Y2~Y1, data = df_it))$coefficients
    obs_T_mat2[tid, i] <- this_lm["Y1", "t value"]
    obs_beta_mat2[tid, i] <- this_lm["Y1", "Estimate"]
  
    # permutation 
    for(m in 1:M){
      rid <- sample(1:nrow(df_it), nrow(df_it), replace = T)
      df_it_boot <- df_it[rid, ]
      boot_lm <-  summary(lm(Y2~Y1, data = df_it_boot))$coefficients
      T_mat2[tid, i, m] <- boot_lm["Y1", "t value"]
      beta_mat2[tid, i, m] <- boot_lm["Y1", "Estimate"]
    }

  }
  
  setTxtProgressBar(pb, i)

}
t2 <- Sys.time()
close(pb)
```


```{r, fig.height=3, fig.width=15}
data.frame(
  time = t,
  obs_beta = obs_beta_mat2[,1], 
  beta_mat2[,1,]) %>%
  pivot_longer(starts_with("X"), names_prefix = "X") %>%
  ggplot()+
  geom_histogram(aes(x=value), bins = 30)+
  geom_vline(aes(xintercept = obs_beta), col = "red")+
  facet_grid(cols = vars(time))+
  labs(title = "Slope estimates")
```

```{r}
# using slope
pval_mat2 <- matrix(NA, nT, N)
for(i in 1:N){
  for(j in 1:nT){
    pval_mat2[j, i] <- mean(abs(beta_mat2[j, i, ]) > abs(obs_beta_mat2[j, i]))
  }
}

data.frame(t = t, reject = apply(pval_mat2 < 0.05, 1, mean))
```

## GLS with spatial correlation


```{r ci_func_gls}
gls_beta_pval <- function(x){
  fit_gls <- gls(Y2~Y1, data = x, correlation = corGaus(value=0.2, ~s1+s2))
  beta1 <- coef(fit_gls)["Y1"] 
  pval <- summary(fit_gls)$tTable["Y1", "p-value"]
  
  return(data.frame(beta1 = beta1, pval = pval))
}
```

```{r, cache=TRUE, eval=FALSE}
t1 <- Sys.time()
df_gls_pval <- df_subj %>%
  # filter(id %in% 1:10) %>%
  # group_by(t) %>%
  group_by(id, t) %>%
  group_modify(~gls_beta_pval(.))
t2 <- Sys.time()
```

```{r, eval=FALSE}
# bind_rows(df_pls_pval) %>%
#   mutate(t = rep(t, each = 10)) %>%
df_gls_pval %>% 
  mutate(reject = (pval <= 0.05)) %>%
  group_by(t) %>%
  summarise(type1err = mean(reject)) 
```

<!-- The GLS process for all subjects at all time points on reduced data took `r round(t2-t1, 3)` hours.  -->

<!-- What if I fit GLS on the whole dataset and do permutation test? The whole dataset takes a really long time to fit, so here I'll start with 10 subject at a single time point.  -->

<!-- ```{r} -->
<!-- t1 <- Sys.time() -->
<!-- fit_gls <- df_subj %>%  -->
<!--   filter(id %in% 1:10) %>% -->
<!--   filter(t==0.2) %>%  -->
<!--   mutate(id <- as.factor(id)) %>% -->
<!--   gls(Y2~Y1, data = ., correlation = corGaus(value=0.2, ~s1+s2|id)) -->
<!-- t2 <- Sys.time() -->

<!-- # t2-t1 -->
<!-- summary(fit_gls) -->
<!-- ``` -->
<!-- The model for 10 subjects took 9 min to fit. Y1 is not significant.  -->

<!-- ## LME -->

<!-- What if I account for the effect of space, using spatial effect as a random effect? -->

<!-- $$Y_{2i}(s_1, s_2, t) =\beta_0 +\beta_1 Y_{1i}(s_1, s_2, t)+b_{0i}+b_{1i}s_1+b_{2i}s_2+b_{3i}t+\epsilon_i(s_1, s_2, t)$$ -->


<!-- ```{r} -->
<!-- t1 <- Sys.time() -->
<!-- fit_lme <- df_subj %>%  -->
<!--   filter(id %in% 1:10) %>% -->
<!--   filter(t==0.2) %>%  -->
<!--   mutate(id <- as.factor(id)) %>% -->
<!--   lme(fixed = Y2~Y1, random = ~s1+s2|id, data = ., correlation = corGaus(value=0.2, ~s1+s2|id)) -->
<!-- t2 <- Sys.time() -->
<!-- ``` -->


<!-- ```{r} -->
<!-- summary(fit_lme) -->
<!-- ``` -->
<!-- Even here, Y1 has a significant relationship with Y2, though the estimate of slope is very very small.  -->