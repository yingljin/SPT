---
title: "Simulating spatial-temporal data"
author: "`r Sys.Date()`"
output: 
  html_document:
    self_contained: yes
    number_sections: true
    code_folding: hide
    toc: true
    toc_depth: 3
    toc_float: true
    font: 12pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

set.seed(1026)

library(mgcv)
library(nlme)
library(tidyverse)
library(knitr)
library(kableExtra)
library(mvtnorm)
library(ggpubr)
library(here)

source(here("Code/RandomField.R"))
```


# Generation framework

The simulation, technically, would be established on a continuous space-time domain, even though the final "observations" would be a discrete realization.

Take a Gaussian process for example:

1. The observation is composed of a true latent process and an error process.

$$Y_i(\mathbf{s}, t) = \eta_i(\mathbf{s}, t) +\epsilon_i(\mathbf{s}, t)$$
2. The true latent process is composed of a fixed process and a random (subject-specific) process. 

$$\eta_i(\mathbf{s}, t) = \mu(\mathbf{s}, t)+b_i(\mathbf{s}, t)$$

- $\mu(\mathbf{s}, t)$ is the population mean function, shared across subjects
- $b_i(\mathbf{s}, t)$ is the individual-level random effect

3. The error process is spatially-correlated. Correlation is introduced through a moving average random field: 

$$\epsilon_i(\mathbf{s}, t) =  \frac{1}{N_r}\sum_{\mathbf{s'} \in S_r}Z(\mathbf{s'}, t)$$


where:

- $S_r$ is a neighborhood around $\mathbf{s}$ where the radius is r
- $N_r$ is the number of spacial points within neighborhood $S_r$
- $Z(\mathbf{s'}, t)$ is a white noise process


4. The second outcome is generated from the first outcome, and the correlation is time-varying

$$Y'_i(\mathbf{s},t) = \beta_i(t)Y_i(\mathbf{s},t) + b'_i(\mathbf{s},t) + \epsilon_i(\mathbf{s}, t)$$


# Full data

Follow up on last time, we would like to generate data from the null hypothesis ($Y_1$, $Y_2$ are not correlated with each other), examine p values from LM or permutation test, and calculate their type I error (probablity of reject under the null hypothesis) at 95% confidence level.

Let's generate from the following scheme: 

\[\begin{aligned}
Y_{1i}(\mathbf{s}, t) &= \phi(\mathbf{s})\xi_{i1}+t\xi_{i2}+\epsilon_{1i}(\mathbf{s}, t), \ \xi_{ik} \sim N(0, \sigma^2_{k}) \\
\phi_1(\mathbf{s}) &= \sqrt{s_1^2+s_2^2}\\
Y_{2i}(\mathbf{s},t) &= \epsilon_{2i}(\mathbf{s}, t)\\
\end{aligned}\]

- $\mathbf{s}$ is the **standardized** spatial index from a 256 by 256 imagae
- $t = 0.2, 0.4, 0.6, 0.8, 1$
- $\xi_{i1}, \xi_{i2}$ are indenpendently generated from $N(0, 1)$
- $\epsilon_{1i}$ and $\epsilon_{2i}$ is a moving average process of a 2D white noise field from $N(0, 5^2)$, derived by a 15 by 15 filter. Please note here I have increased the variation of white noise Z because I would like to keep strong spatial correlation (per last meeting)
- I removed the random effects from $Y_2$, because it is easier to find true slope this way. Also I was worried it might induce unidentifiability problem. 

In the real dataset, a lot of things would be different for each lesion (e.g., the correlation, size of image, center of image, correlation between two outcomes). However, the current set up seems to imply constant correlation across subjects.


```{r grid}
# set up spcae-time grid
# generate a 2D image of 256 by 256
sid1 <- sid2 <- 1:256
nS <- 256
df_grid <- expand_grid(sid1, sid2) %>%
  mutate(s1 = as.vector(scale(sid1)), s2 = as.vector(scale(sid2))) %>% 
  mutate(dist = sqrt(s1^2+s2^2))
## we would need distance to center for the random effect of Y1

# times of scan
t <- seq(0.2, 1 , by = 0.2)
nT <- length(t)

df_grid <- expand_grid(df_grid, t=t) 
## 256^2*5 = 327680 observations for each subject
```


```{r sim_setup}
N <- 100 # sample size
K1 <- 2 # number of coefficients in random effect of Y1
xi_mat <- mvtnorm::rmvnorm(N, mean = rep(0, K1), sigma = diag(rep(1, K1)))

# container
df_subj <- expand_grid(id = 1:N, df_grid)
df_subj$Y1 <- df_subj$Y2 <- NA
## N * 256^2 * nT = 3276800 obesrvations in total

# kernel size for moving average
ma_size <- 15

```


```{r gen_data, results='hide', cache=TRUE}
pb <- txtProgressBar(min=0, max=N, style = 3)

t1 <- Sys.time()
for(i in 1:N){ # fix a subject
  
  xi_i <- xi_mat[i, ]
  
  for(this_t in t){ # fix a time point
    
    dist_it <- df_subj$dist[df_subj$id==i & df_subj$t==this_t]
    
    # generate Y1
    ## a moving average error
    Zmat_it1 <- matrix(rnorm(nS^2, 0, 5), nS, nS)
    MA_err_it1 <- MA_rand_field(ma_size, Zmat_it1) # flatten by column
    MA_err_it1 <- as.vector(MA_err_it1)
    # outcome
    Y1_it <- dist_it*xi_i[1]+this_t*xi_i[2]+MA_err_it1
    df_subj$Y1[df_subj$id==i & df_subj$t==this_t] <- Y1_it
    
    # generate Y2
    ## a moving average error
    Zmat_it2 <- matrix(rnorm(nS^2, 0, 5), nS, nS)
    MA_err_it2 <- MA_rand_field(ma_size, Zmat_it2) # flatten by column
    Y2_it <- as.vector(MA_err_it2)
    df_subj$Y2[df_subj$id==i & df_subj$t==this_t] <- Y2_it
  }

setTxtProgressBar(pb, i)
}
t2 <- Sys.time()

close(pb)
```


It took `r round(t2-t1, 3)` minutes to generate data for `r N` subjects.

Below is an example of one subject: 

```{r, fig.height=3, fig.width=15}
df_subj %>% 
  filter(id==25) %>% 
  ggplot()+
  geom_tile(aes(x=s1, y=s2, fill = Y1))+
  facet_wrap(~t, nrow = 1)+
  labs(title = "Y1")

df_subj %>% 
  filter(id==25) %>% 
  ggplot()+
  geom_tile(aes(x=s1, y=s2, fill = Y2))+
  facet_wrap(~t, nrow = 1)+
  labs(title = "Y2")

df_subj %>% 
  filter(id==25) %>%
  ggplot(aes(x=Y1, y=Y2))+
  geom_point(size = 0.2)+
  geom_smooth(formula = 'y~x', method = "lm")+
  stat_cor(method = "pearson")+
  facet_wrap(~t, nrow = 1)
```



## LM

Below I'd like to fit a linear regression model of $Y_{2i}$ wrt $Y_{it}$, get the p values of slope and calculate type I error. The model expression is as below: 

$$Y_{2i}(\mathbf{s}|t) = \beta_{0ti}+\beta_{1ti}Y_{1i}(\mathbf{s}|t)+\epsilon_i(\mathbf{s}|t)$$



- We fit a linear model at **each time** point for **each subject** (that is, $5 \times 100$ models)
- The true slope in this case is 0. The two outcomes are not correlated with each other. 


```{r func_lm}
lm_beta_pval <- function(x){
  fit_lm <- lm(Y2~Y1, data = x)
  beta1 <- coef(fit_lm)["Y1"] 
  pval <- summary(fit_lm)$coefficients["Y1", "Pr(>|t|)"]
  
  return(data.frame(beta1 = beta1, pval = pval))
}
```

```{r}
df_subj %>% 
  group_by(id, t) %>%
  group_modify(~lm_beta_pval(.)) %>%
  mutate(reject = (pval <= 0.05)) %>%
  group_by(t) %>%
  summarise(type1err = mean(reject)) 
```
This type I error is so high! Is it solely because the correlation introduced by both sptailly-correlated error term? 

## Permutation test 

I have decided to code up my own permutation test because the packages (coin, lmPerm) either couldn't handle the data or have a lot of other contingencies. 

I followed this [reference](https://bookdown.org/curleyjp0/psy317l_guides5/permutation-testing.html#correlation-coefficient-permutation-tests) on permutation test of correlation. 



```{r perm_test, class.source='fold-show', results='hide', cache=TRUE}

M <- 1000 # number of permutation
pval_mat <- matrix(NA, nrow = N, ncol = nT)
pb <- txtProgressBar(min = 0, max=N, style = 3)

t1 <- Sys.time()
for(i in 1: N){
  for(this_t in seq_along(t)){
    df_it <- df_subj %>% filter(id==i & t == t[this_t])
    
    # observed slope
    obs_beta <- coef(lm(Y2~Y1, data = df_it))["Y1"]
    
    # permutation 
    beta_perm <- rep(NA, M)
    for(m in 1:M){
      df_it_m <- df_it
      df_it_m$Y2_perm <- sample(df_it_m$Y2) # shuffle Y2, fix Y1
      beta_perm[m] <- coef(lm(Y2_perm~Y1, data = df_it_m))["Y1"]
    }
    
    pval_mat[i, this_t] <- sum(beta_perm>obs_beta)/M
    
  }
  
  setTxtProgressBar(pb, i)
  
  
}
t2 <- Sys.time()
close(pb)


```

The permutation rest took `r round(t2-t1, 2)` minutes. 

```{r}
data.frame(t = t, 
           type1err = apply(pval_mat < 0.05, 2, mean))
```


# Reduced data

Since the data size is too large for GLS, I'll do another experiment on the reduced data. Along each axis, one grid point is taken every 8 grid points, making the reduced an 32 by 32 image at each time point for each subject. 

```{r}
# reduce grid
re_sid <- seq(0, 256, by = 8)
df_subj_re <- df_subj %>% filter(sid1 %in% re_sid & sid2 %in% re_sid)
```



## LM

```{r}
df_subj_re %>% group_by(id, t) %>%
  group_modify(~lm_beta_pval(.)) %>%
  mutate(reject = (pval <= 0.05)) %>%
  group_by(t) %>%
  summarise(type1err = mean(reject)) 
```

## Permutation test

```{r, class.source='fold-show', results='hide', cache=TRUE}

M <- 1000 # number of permutation
pval_mat_re <- matrix(NA, nrow = N, ncol = nT)
pb <- txtProgressBar(min = 0, max=N, style = 3)

t1 <- Sys.time()
for(i in 1: N){
  for(this_t in seq_along(t)){
    df_it <- df_subj_re %>% filter(id==i & t == t[this_t])
    
    # observed slope
    obs_beta <- coef(lm(Y2~Y1, data = df_it))["Y1"]
    
    # permutation 
    beta_perm <- rep(NA, M)
    for(m in 1:M){
      df_it_m <- df_it
      df_it_m$Y2_perm <- sample(df_it_m$Y2) # shuffle Y2, fix Y1
      beta_perm[m] <- coef(lm(Y2_perm~Y1, data = df_it_m))["Y1"]
    }
    
    pval_mat_re[i, this_t] <- sum(beta_perm>obs_beta)/M
    
  }
  
  setTxtProgressBar(pb, i)
  
  
}
t2 <- Sys.time()
close(pb)
```

The permutation rest took `r round(t2-t1, 2)` seconds. 

```{r}
data.frame(t = t, 
           type1err= apply(pval_mat_re < 0.05, 2, mean))
```

## GLS with spatial correlation


```{r ci_func_gls}
gls_beta_pval <- function(x){
  fit_gls <- gls(Y2~Y1, data = x, correlation = corGaus(value=0.2, ~s1+s2))
  beta1 <- coef(fit_gls)["Y1"] 
  pval <- summary(fit_gls)$tTable["Y1", "p-value"]
  
  return(data.frame(beta1 = beta1, pval = pval))
}
```

```{r, cache=TRUE}
t1 <- Sys.time()
df_gls_pval <- df_subj_re %>%
  # filter(id==1) %>%
  # group_by(t) %>%
  group_by(id, t) %>%
  group_modify(~gls_beta_pval(.)) 
t2 <- Sys.time()
```

```{r}
df_gls_pval %>% 
  mutate(reject = (pval <= 0.05)) %>%
  group_by(t) %>%
  summarise(type1err = mean(reject)) 
```

The GLS process for all subjects at all time points on reduced data took `r round(t2-t1, 3)` hours. 




