---
title: "Simulating spatial-temporal data"
author: "`r Sys.Date()`"
output: 
  html_document:
    self_contained: yes
    number_sections: true
    code_folding: hide
    toc: true
    toc_depth: 3
    toc_float: true
    font: 12pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

set.seed(305)

library(mgcv)
library(nlme)
library(tidyverse)
library(knitr)
library(kableExtra)
library(mvtnorm)
library(ggpubr)
library(here)
library(gamm4)

source(here("Code/RandomField.R"))
```


# Generation framework

The simulation, technically, would be established on a continuous space-time domain, even though the final "observations" would be a discrete realization.

Take a Gaussian process for example:

1. The observation is composed of a true latent process and an error process.

$$Y_i(\mathbf{s}, t) = \eta_i(\mathbf{s}, t) +\epsilon_i(\mathbf{s}, t)$$
2. The true latent process is composed of a fixed process and a random (subject-specific) process. 

$$\eta_i(\mathbf{s}, t) = \mu(\mathbf{s}, t)+b_i(\mathbf{s}, t)$$

- $\mu(\mathbf{s}, t)$ is the population mean function, shared across subjects
- $b_i(\mathbf{s}, t)$ is the individual-level random effect

3. The error process is spatially-correlated. Correlation is introduced through a moving average random field: 

$$\epsilon_i(\mathbf{s}, t) =  \frac{1}{N_r}\sum_{\mathbf{s'} \in S_r}Z(\mathbf{s'}, t)$$


where:

- $S_r$ is a neighborhood around $\mathbf{s}$ where the radius is r
- $N_r$ is the number of spacial points within neighborhood $S_r$
- $Z(\mathbf{s'}, t)$ is a white noise process


4. The second outcome is generated from the first outcome, and the correlation is time-varying

$$Y'_i(\mathbf{s},t) = \beta_i(t)Y_i(\mathbf{s},t) + b'_i(\mathbf{s},t) + \epsilon_i(\mathbf{s}, t)$$


# Generate data under the null assumption

Follow up on last time, we would like to generate data from the null hypothesis where $Y_1$, $Y_2$ are not correlated with each other. We will use a few methods to test their association and calculate the type I error.

We generate data from the following scheme: 

$$\begin{aligned}
Y_{1i}(s_1, s_2, t) &=\xi_{i1}\phi(s_1, s_2)+\xi_{i2}t+\epsilon_{1i}(s_1, s_2, t) \\
Y_{2i}(s_1, s_2, t) &=\zeta_{i1}\phi(s_1, s_2)+\zeta_{i2}t+\epsilon_{2i}(s_1, s_2, t) \\
\xi_{ik}, \zeta_{ik} & \sim N(0, 1) \\
\end{aligned}$$

and $\epsilon_{i1}$, $\epsilon_{i2}$ are moving average generated from different kernal size: $r_1=5$, $r_2=3$.    


```{r grid}
# set up spcae-time grid
# generate a 2D image of 256 by 256
sid1 <- sid2 <- 1:32
nS <- 32
df_grid <- expand_grid(sid1, sid2) %>%
  mutate(s1 = as.vector(scale(sid1)), s2 = as.vector(scale(sid2))) %>% 
  mutate(dist = sqrt(s1^2+s2^2))
## we would need distance to center for the random effect of Y1

# times of scan
t <- seq(0.2, 1 , by = 0.2)
nT <- length(t)

df_grid <- expand_grid(df_grid, t=t) 
## 32^2*5 = 5120 observations for each subject
```


```{r sim_setup}
N <- 100 # sample size

# container
df_subj <- expand_grid(id = 1:N, df_grid)
df_subj$Y1 <- df_subj$Y2 <- NA
## N * 256^2 * nT = 512000 obesrvations in total

# kernel size for moving average
ma_size1 <- 5
ma_size2 <- 3
```


```{r gen_data1, results='hide'}
# generate individual scores
true_xi <- matrix(rnorm(2*N, 0, 1.5), nrow = N, ncol = 2)
true_zeta <- matrix(rnorm(2*N, 0, 1.5), nrow = N, ncol = 2)

# generate outcomes
pb <- txtProgressBar(min=0, max=N, style = 3)

t1 <- Sys.time()
for(i in 1:N){ # fix a subject
  
  for(this_t in t){ # fix a time point
    
    # random effect of this subject at this time
    dist_it <- df_subj$dist[df_subj$id==i & df_subj$t==this_t]
  
    # generate Y1
    ## a moving average error
    Zmat_it1 <- matrix(rnorm(nS^2, 0, 1), nS, nS)
    ma_err1 <- MA_rand_field(ma_size1, Zmat_it1)
    y1_it <- true_xi[i,1]*dist_it+true_xi[i,2]*this_t + as.vector(ma_err1)
    df_subj$Y1[df_subj$id==i & df_subj$t==this_t] <- y1_it
    
    # generate Y2
    ## a moving average error
    Zmat_it2 <- matrix(rnorm(nS^2, 0, 1), nS, nS)
    ma_err2 <- MA_rand_field(ma_size2, Zmat_it2)
    y2_it <- true_zeta[i,1]*dist_it+true_zeta[i,2]*this_t + as.vector(ma_err2)
    df_subj$Y2[df_subj$id==i & df_subj$t==this_t] <- y2_it
  }

setTxtProgressBar(pb, i)
}
t2 <- Sys.time()

close(pb)
```


It took `r round(t2-t1, 3)` minutes to generate data for `r N` subjects. Below we show an example of one subject. 


```{r, example_data, fig.height=6, fig.width=15}
df_subj %>% 
  filter(id==15) %>%
  pivot_longer(starts_with("Y")) %>%
  ggplot()+
  geom_tile(aes(x=s1, y=s2, fill = value))+
  facet_grid(cols=vars(t), rows = vars(name))+
  labs(title = "Outcome")
```

```{r}
# score of Y1
true_xi[15, ]

# score of Y2
true_zeta[15, ]
```

From the last meeting, we know that premutation test is not robust against spatial correlation and does no better than regular linear regression. So this time, we would like to try bootstrap instead.

# Inference conditioning on time and subject


## LM

```{r, fig.height=12, fig.width=15}
df_subj %>% 
  filter(id %in% sample(1:N, size = 4)) %>%
  ggplot(aes(x=Y1, y=Y2))+
  geom_point(size = 0.2)+
  geom_smooth(formula = 'y~x', method = "lm")+
  stat_cor(method = "pearson")+
  facet_grid(rows = vars(id), cols = vars(t))+
  labs(title = "Correlation")
```

We fit a simple model across space conditioning on each subject and time:

$$Y_{2i}(\mathbf{s}|t) = \beta_{0ti}+\beta_{1ti}Y_{1i}(\mathbf{s}|t)+\epsilon_i(\mathbf{s}|t)$$

```{r}
df_subj %>%
  group_by(id, t) %>%
  group_modify(~{
    fit_lm <- lm(Y2~Y1, data = .x)
    data.frame(beta = coef(fit_lm)["Y1"], 
               pval = summary(fit_lm)$coefficients["Y1", "Pr(>|t|)"])
  }) %>%
  mutate(reject = pval < 0.05) %>%
  group_by(t) %>%
  summarise_at("reject", mean)
```


This type I error is very high! It almost always reject the null hypothesis even thought it is true.

From now on there are two ways we can think about this problem:
1. Still conditioning on time and subject, boostrap over space
2. Pool space and time, fit lme, do permutation/bootstrap test


## Conditional boostrap

### Single pixel

Here, we still condition on time and subject. For each subject at each time point, we can sample with replacement either pixels or blocks of pixels, fit LM, find distribution of slope estimates and calculate p values.

```{r bootstrap_pixel, results='hide', cache=TRUE}
M <- 1000 # number of boostraps

# sample single pixel
obs_beta_mat <- array(NA, dim = c(nT, N)) # dims: time, subject
boot_beta_mat <- array(NA, dim = c(nT, N, M)) # dims: time, subject, permutation

# bootstrap
pb <- txtProgressBar(0, N, style = 3)
t1 <- Sys.time()

for(i in 1: N){
  for(tid in seq_along(t)){
    this_t <- t[tid]
    df_it <- df_subj %>% filter(id==i & t==this_t)
    
    # observed 
    this_lm <- summary(lm(Y2~Y1, data = df_it))$coefficients
    obs_beta_mat[tid, i] <- this_lm["Y1", "Estimate"]
  
    # permutation 
    for(m in 1:M){
      rid <- sample(1:nrow(df_it), nrow(df_it), replace = T)
      df_it_boot <- df_it[rid, ]
      boot_lm <-  summary(lm(Y2~Y1, data = df_it_boot))$coefficients
      boot_beta_mat[tid, i, m] <- boot_lm["Y1", "Estimate"]
    }

  }
  
  setTxtProgressBar(pb, i)

}
t2 <- Sys.time()
close(pb)
```

The bootstrap procedure took `r round(t2-t1, 3)` minutes. 

```{r boot_pixel_example, fig.height=3, fig.width=15}
data.frame(
  time = t,
  obs_beta = obs_beta_mat[, 1], 
  true_beta = 0,
  boot_beta_mat[,1,]) %>%
  pivot_longer(starts_with("X"), names_prefix = "X") %>%
  ggplot()+
  geom_histogram(aes(x=value), bins = 30)+
  geom_vline(aes(xintercept = obs_beta, col = "Observed"))+
  geom_vline(aes(xintercept = true_beta, col = "Ture"))+
  facet_grid(cols = vars(time))+
  labs(title = "Slope estimates")
```

```{r boot_pval_pixel}
# test conclusion
boot_reject_mat <- matrix(NA, nT, N)
for(i in 1:N){
  for(j in 1:nT){
    cutoff <- quantile(boot_beta_mat[j, i, ], probs = c(0.025, 0.975))
    boot_reject_mat[j, i] <- (0 < cutoff[1]) | (0 > cutoff[2])
  }
}

data.frame(t = t, typeIerror = apply(boot_reject_mat, 1, mean))
```

Bootstrap single pixel did not do anything to improve type I error. This is probably for the same reason of permutation-spatial correlation is broken when sample with replacement. 

### Block

I'd like to sample 4 by 4 block of pixels. For now, the block size needs to be the divisor of image size, because I am not sure what to do with the edge blocks otherwise. 

```{r block_boot, results='hide', cache=TRUE}
M <- 1000 # number of boostraps
b <- 8 # block size
boot_beta_block <- array(NA, dim = c(nT, N, M)) # dims: time, subject, permutation

# bootstrap
pb <- txtProgressBar(0, N, style = 3)
t1 <- Sys.time()

for(i in 1: N){
  for(tid in seq_along(t)){
    this_t <- t[tid]
    
    # A matrix of observed outcome
    df_it <- df_subj %>% filter(id==i & t==this_t)
    Y_mat <- matrix(df_it$Y2, nS, nS, byrow = T)
    
    # divide block
    rblock <- (row(Y_mat)-1)%/%b+1
    cblock <- (col(Y_mat)-1)%/%b+1
    block_id_mat <- (rblock-1)*max(cblock) + cblock
    df_it$block_id <- as.vector(t(block_id_mat))
    block_list <- split(df_it, f = df_it$block_id)
      
    # block boostrap
    for(m in 1:M){
    
      # sample block
      nblock <- max(block_id_mat)
      df_it_boot<- bind_rows(block_list[sample(1:nblock, size = nblock, replace = T)])
      
      # regression
      boot_lm <-  summary(lm(Y2~Y1, data = df_it_boot))$coefficients
      boot_beta_block[tid, i, m] <- boot_lm["Y1", "Estimate"]
    }

  }
  
  setTxtProgressBar(pb, i)

}
t2 <- Sys.time()
close(pb)

```

```{r boot_block_example, fig.height=3, fig.width=15}
data.frame(
  time = t,
  obs_beta = obs_beta_mat[, 1], 
  true_beta = 0,
  boot_beta_block[,1,]) %>%
  pivot_longer(starts_with("X"), names_prefix = "X") %>%
  ggplot()+
  geom_histogram(aes(x=value), bins = 30)+
  geom_vline(aes(xintercept = obs_beta, col = "Observed"))+
  geom_vline(aes(xintercept = true_beta, col = "True"))+
  facet_grid(cols = vars(time))+
  labs(title = "Slope estimates")
```


```{r boot_pval_block}
# test conclusion
block_boot_reject_mat <- matrix(NA, nT, N)
for(i in 1:N){
  for(j in 1:nT){
    cutoff <- quantile(boot_beta_block[j, i, ], probs = c(0.025, 0.975))
    block_boot_reject_mat[j, i] <- (0 < cutoff[1]) | (0 > cutoff[2])
  }
}

data.frame(t = t, typeIerror = apply(block_boot_reject_mat, 1, mean))
```

Looks like the block bootstrap also didn't do much. The type I error only increased a little bit. 

I am starting to think it is not possible to improve type I error based on simple linear regression. It is necessary to account for spatial effect in the model. The GLS with Gaussian spatial error took too long, so I would like to start with a mixed model. 

If we continue to fix time and subject, I am not sure what we can do. If we 


## GLS

```{r gls, cache=TRUE}
t1 <- Sys.time()
all_fit_gls <- df_subj %>%
  # filter(id%in%1:10) %>%
  group_by(id, t) %>%
  group_modify(~{fit_gls <- gls(Y2~Y1, data = .x,
                                correlation = corExp(value = 0.2, form = ~s1+s2))
                 data.frame(beta_est = fit_gls$coefficients["Y1"],
                            pval = summary(fit_gls)$tTable["Y1", "p-value"])})
t2 <- Sys.time()
```

Time: `r t2-t1` hours

```{r}
all_fit_gls%>%
  mutate(reject= pval< 0.05) %>%
  group_by(t) %>% 
  summarize(typeIerror=mean(reject))
```


Even if accounting for spatial correlation below, the model still leads to false rejection. 

## GAM

What if we introduce very complex spatial effect using spline basis? Let's fit a gam model for each subject at each time point. Model formula as below:

$$Y_{2i}(s_1, s_2|t)=\beta_{0it}+\beta_{1it}Y_{1i}(s_1, s_2|t)+f_{it}(s_1)+f_{it}(s_2)+g_{it}(s_1, s_2)$$



```{r gam, cache=TRUE}
t1 <- Sys.time()
all_fit_gam <- df_subj %>%
  group_by(id, t) %>%
  group_modify(~{fit_gam <- gam(Y2 ~ Y1+s(s1, k = 20)+s(s2, k = 20) + ti(s1, s2, k = 20), data = .x)
                 data.frame(beta_est = fit_gam$coefficients["Y1"],
                            pval = summary(fit_gam)$p.pv["Y1"])})
t2 <- Sys.time()
```

```{r}
all_fit_gam %>%
  mutate(reject=pval< 0.05) %>%
  group_by(t) %>% 
  summarize(typeIerror=mean(reject))
```

Time: `r t2-t1` hours

# Inference pooling over time and subject

If repeated models conditioning on time and subject does not reach the nominal value of type I error, would pooling over time and subject be better? We need to fit a model taking into account all the data, as well as spatial correlation and time dependence. 


## LME

```{r, error=TRUE, class.source='fold-show'}
lme_pool <- lme(fixed = Y2~Y1+s1+s2+t, 
                random = ~s1+s2+t|id,
                data=df_subj,
                correlation = corExp(value=0.2, form=~s1+s2|id))
```

The mixed model with spatial correlation was not able to fit. It also failed when we include the interaction of spatial index. 

```{r lme_pool, error=TRUE, class.source='fold-show'}
lme_pool <- lme(fixed = Y2~Y1+s1+s2+t, 
                random = ~s1+s2+t|id,
                data=df_subj)
summary(lme_pool)
```

This model clearly leads to the false rejection of $Y_1$ and the false acceptance of space and time index.

The permutation test of this model would be the Freedman-Lane procedure as Amy talked about. However, I doubt a linear effect would be enough for the complex spatial correlation as in this data. I am not sure if bootstrap/permutation of extremely flexible model would work. 


## GAM with very flexible spatial effect

Anyways, let's try to fit a flexible model first. This is actually a very complicated problem. I think I need to create the design maatrix first and then fit an LME

```{r}
# set up design matrix

```


```{r, error=TRUE, class.source='fold-show'}
df_subj$id <- as.factor(df_subj$id)
unique(df_subj$id)
gamm_pool <- gamm4(Y2~s(Y1)+t2(s1, s2)+s(t, k=3), 
      random = ~(s1+s2+t|id),
      data = df_subj)
# 
# gam_pool <- gam(Y2~s(Y1)+t2(s1, s2)+s(t, k=3)+
#                   s(id, t, by=s1, bs="re")+
#                   s(id, t, by=s2, bs="re")+
#                   s(id, by=t, bs="re", k=3), 
#                 data = df_subj)
summary(gamm_pool)
```

Looks like this model is still not sufficient enough to identify the null effect of $Y_1$. 